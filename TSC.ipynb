{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "from pathlib import Path\n",
    "from fastai.basics import *\n",
    "path = Path(\"UCRArchive_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('UCRArchive_2018/car/Car_TEST.tsv'),\n",
       " WindowsPath('UCRArchive_2018/car/Car_TRAIN.tsv'),\n",
       " WindowsPath('UCRArchive_2018/car/desktop.ini'),\n",
       " WindowsPath('UCRArchive_2018/car/README.md')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((path/\"car\").iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSCDS(torch.utils.data.Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.items = [1,2,3]\n",
    "    \n",
    "    def __len__(self): return len(self.X)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return np.expand_dims(self.X[idx],axis=0), self.Y[idx] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self,inputC,outputC,kernelSize):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(inputC,outputC,kernelSize)\n",
    "        self.drop = torch.nn.Dropout()\n",
    "        self.bn = torch.nn.BatchNorm1d(outputC)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.drop(self.bn(torch.relu(self.conv(x))))\n",
    "\n",
    "class TSCModel(torch.nn.Module):\n",
    "    def __init__(self,numClasses):\n",
    "        super().__init__()\n",
    "        self.blocks = torch.nn.ModuleList([ConvBlock(i,o,k) for i,o,k in [(1,128,8),(128,256,5),(256,128,3)]])\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.out = torch.nn.Linear(128,numClasses)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.float()\n",
    "        for b in self.blocks: x = b(x)\n",
    "        x = self.avgpool(x)\n",
    "        return self.out(x.squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(d,weights=None,epochs=20,lr=5e-2):\n",
    "    trainDF = pd.read_csv(path/f\"{d}/{d}_TRAIN.tsv\",sep=\"\\t\",header=None)\n",
    "    testDF = pd.read_csv(path/f\"{d}/{d}_TEST.tsv\",sep=\"\\t\",header=None)\n",
    "    cat = trainDF.iloc[:,0].astype(\"category\")\n",
    "    trainDF.iloc[:,0] = cat.cat.codes\n",
    "    testDF.iloc[:,0] = pd.Categorical(testDF.iloc[:,0],categories=cat.cat.categories).codes\n",
    "    nClasses = len(cat.cat.categories)\n",
    "    \n",
    "    trainDS = TSCDS(trainDF.iloc[:,1:].values,trainDF.iloc[:,0].values)\n",
    "    testDS = TSCDS(testDF.iloc[:,1:].values,testDF.iloc[:,0].values)\n",
    "    model = TSCModel(nClasses)\n",
    "    learn = Learner(data,model,loss_func=torch.nn.functional.cross_entropy,metrics=[accuracy])\n",
    "    if weights:\n",
    "        learn.model.blocks.load_state_dict(weights)\n",
    "    \n",
    "    learn.fit_one_cycle(epochs,lr)\n",
    "    return learn.model.blocks.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='20', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      45.00% [9/20 00:03<00:03]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.749991</th>\n",
       "    <th>1.465224</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.574655</th>\n",
       "    <th>1.504321</th>\n",
       "    <th>0.216667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.503467</th>\n",
       "    <th>2.492872</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.515240</th>\n",
       "    <th>1.740129</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.459075</th>\n",
       "    <th>1.433599</th>\n",
       "    <th>0.333333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.467439</th>\n",
       "    <th>1.397818</th>\n",
       "    <th>0.350000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.421116</th>\n",
       "    <th>2.365916</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>1.423876</th>\n",
       "    <th>1.537795</th>\n",
       "    <th>0.283333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>1.423254</th>\n",
       "    <th>1.377244</th>\n",
       "    <th>0.316667</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='30', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-5b24023572d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-192-d9c0a5e22de0>\u001b[0m in \u001b[0;36mtrainNetwork\u001b[1;34m(d, weights, epochs, lr)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[1;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[0;32m     20\u001b[0m                                         pct_start=pct_start, **kwargs))\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[1;32m--> 166\u001b[1;33m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m->\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'valid_dl'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_dl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 val_loss = validate(model, data.valid_dl, loss_func=loss_func,\n\u001b[1;32m---> 89\u001b[1;33m                                        cb_handler=cb_handler, pbar=pbar)\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mnums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0myb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-191-3479d46a5164>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights = None\n",
    "for p in path.iterdir():\n",
    "    if p.is_dir():\n",
    "        weights = trainNetwork(p.name,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:35 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.615551</th>\n",
       "    <th>1.568383</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.589636</th>\n",
       "    <th>1.557210</th>\n",
       "    <th>0.333333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.563373</th>\n",
       "    <th>1.455474</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.526425</th>\n",
       "    <th>1.336305</th>\n",
       "    <th>0.500000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.481152</th>\n",
       "    <th>1.226602</th>\n",
       "    <th>0.633333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.417295</th>\n",
       "    <th>1.329268</th>\n",
       "    <th>0.566667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.360600</th>\n",
       "    <th>1.359261</th>\n",
       "    <th>0.300000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>1.303365</th>\n",
       "    <th>1.243345</th>\n",
       "    <th>0.516667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>1.257618</th>\n",
       "    <th>1.069968</th>\n",
       "    <th>0.616667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>1.208771</th>\n",
       "    <th>1.193387</th>\n",
       "    <th>0.483333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>1.233717</th>\n",
       "    <th>1.366392</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>1.249702</th>\n",
       "    <th>1.334387</th>\n",
       "    <th>0.383333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>1.242248</th>\n",
       "    <th>1.270352</th>\n",
       "    <th>0.400000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>1.198372</th>\n",
       "    <th>1.997015</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>1.153669</th>\n",
       "    <th>1.050262</th>\n",
       "    <th>0.533333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>1.093696</th>\n",
       "    <th>1.032445</th>\n",
       "    <th>0.600000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>1.187007</th>\n",
       "    <th>0.979278</th>\n",
       "    <th>0.550000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>1.216366</th>\n",
       "    <th>2.377897</th>\n",
       "    <th>0.316667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>1.215010</th>\n",
       "    <th>1.346071</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>20</th>\n",
       "    <th>1.232472</th>\n",
       "    <th>1.590836</th>\n",
       "    <th>0.400000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>21</th>\n",
       "    <th>1.162961</th>\n",
       "    <th>1.145141</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>22</th>\n",
       "    <th>1.188477</th>\n",
       "    <th>1.642459</th>\n",
       "    <th>0.383333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>23</th>\n",
       "    <th>1.196061</th>\n",
       "    <th>1.557037</th>\n",
       "    <th>0.366667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>24</th>\n",
       "    <th>1.212500</th>\n",
       "    <th>1.689725</th>\n",
       "    <th>0.266667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>25</th>\n",
       "    <th>1.187407</th>\n",
       "    <th>1.083794</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>26</th>\n",
       "    <th>1.193559</th>\n",
       "    <th>1.323884</th>\n",
       "    <th>0.350000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>27</th>\n",
       "    <th>1.174371</th>\n",
       "    <th>1.547679</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>28</th>\n",
       "    <th>1.146998</th>\n",
       "    <th>1.143000</th>\n",
       "    <th>0.583333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>29</th>\n",
       "    <th>1.166678</th>\n",
       "    <th>1.306656</th>\n",
       "    <th>0.550000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>30</th>\n",
       "    <th>1.178457</th>\n",
       "    <th>1.427772</th>\n",
       "    <th>0.350000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>31</th>\n",
       "    <th>1.190176</th>\n",
       "    <th>2.340953</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>32</th>\n",
       "    <th>1.220153</th>\n",
       "    <th>1.795185</th>\n",
       "    <th>0.350000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>33</th>\n",
       "    <th>1.198130</th>\n",
       "    <th>2.016839</th>\n",
       "    <th>0.300000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>34</th>\n",
       "    <th>1.241860</th>\n",
       "    <th>1.599926</th>\n",
       "    <th>0.333333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>35</th>\n",
       "    <th>1.226187</th>\n",
       "    <th>1.957110</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>36</th>\n",
       "    <th>1.238088</th>\n",
       "    <th>1.962305</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>37</th>\n",
       "    <th>1.232003</th>\n",
       "    <th>1.289822</th>\n",
       "    <th>0.333333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>38</th>\n",
       "    <th>1.169063</th>\n",
       "    <th>1.515915</th>\n",
       "    <th>0.400000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>39</th>\n",
       "    <th>1.173839</th>\n",
       "    <th>1.825251</th>\n",
       "    <th>0.366667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>40</th>\n",
       "    <th>1.192527</th>\n",
       "    <th>1.347082</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>41</th>\n",
       "    <th>1.167190</th>\n",
       "    <th>1.448692</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>42</th>\n",
       "    <th>1.201205</th>\n",
       "    <th>1.225757</th>\n",
       "    <th>0.533333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>43</th>\n",
       "    <th>1.155067</th>\n",
       "    <th>1.927499</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>44</th>\n",
       "    <th>1.110300</th>\n",
       "    <th>2.206026</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>45</th>\n",
       "    <th>1.105171</th>\n",
       "    <th>1.801435</th>\n",
       "    <th>0.200000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>46</th>\n",
       "    <th>1.145720</th>\n",
       "    <th>1.051880</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>47</th>\n",
       "    <th>1.155632</th>\n",
       "    <th>1.811285</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>48</th>\n",
       "    <th>1.096927</th>\n",
       "    <th>1.194518</th>\n",
       "    <th>0.533333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>49</th>\n",
       "    <th>1.088593</th>\n",
       "    <th>3.460953</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>50</th>\n",
       "    <th>1.145383</th>\n",
       "    <th>1.069075</th>\n",
       "    <th>0.500000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>51</th>\n",
       "    <th>1.175519</th>\n",
       "    <th>1.337830</th>\n",
       "    <th>0.350000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>52</th>\n",
       "    <th>1.156374</th>\n",
       "    <th>0.876681</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>53</th>\n",
       "    <th>1.144525</th>\n",
       "    <th>1.171698</th>\n",
       "    <th>0.400000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>54</th>\n",
       "    <th>1.070745</th>\n",
       "    <th>0.938409</th>\n",
       "    <th>0.616667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>55</th>\n",
       "    <th>1.077085</th>\n",
       "    <th>1.266402</th>\n",
       "    <th>0.400000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>56</th>\n",
       "    <th>1.114236</th>\n",
       "    <th>2.326062</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>57</th>\n",
       "    <th>1.004690</th>\n",
       "    <th>1.098688</th>\n",
       "    <th>0.550000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>58</th>\n",
       "    <th>1.073233</th>\n",
       "    <th>0.992366</th>\n",
       "    <th>0.633333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>59</th>\n",
       "    <th>1.020281</th>\n",
       "    <th>1.270277</th>\n",
       "    <th>0.533333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>60</th>\n",
       "    <th>1.104294</th>\n",
       "    <th>1.111963</th>\n",
       "    <th>0.600000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>61</th>\n",
       "    <th>1.097089</th>\n",
       "    <th>1.055930</th>\n",
       "    <th>0.550000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>62</th>\n",
       "    <th>1.039391</th>\n",
       "    <th>3.313999</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>63</th>\n",
       "    <th>1.007846</th>\n",
       "    <th>1.058527</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>64</th>\n",
       "    <th>0.999835</th>\n",
       "    <th>0.991489</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>65</th>\n",
       "    <th>0.997179</th>\n",
       "    <th>0.927160</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>66</th>\n",
       "    <th>0.958760</th>\n",
       "    <th>0.942371</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>67</th>\n",
       "    <th>0.887612</th>\n",
       "    <th>1.012384</th>\n",
       "    <th>0.583333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>68</th>\n",
       "    <th>0.905010</th>\n",
       "    <th>1.205708</th>\n",
       "    <th>0.600000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>69</th>\n",
       "    <th>0.907135</th>\n",
       "    <th>1.136248</th>\n",
       "    <th>0.600000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>70</th>\n",
       "    <th>1.022429</th>\n",
       "    <th>1.117800</th>\n",
       "    <th>0.666667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>71</th>\n",
       "    <th>0.996183</th>\n",
       "    <th>0.794599</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>72</th>\n",
       "    <th>1.072672</th>\n",
       "    <th>0.799433</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>73</th>\n",
       "    <th>1.041731</th>\n",
       "    <th>1.143423</th>\n",
       "    <th>0.550000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>74</th>\n",
       "    <th>1.045072</th>\n",
       "    <th>0.945977</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>75</th>\n",
       "    <th>0.959655</th>\n",
       "    <th>1.152286</th>\n",
       "    <th>0.633333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>76</th>\n",
       "    <th>0.962949</th>\n",
       "    <th>1.005037</th>\n",
       "    <th>0.633333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>77</th>\n",
       "    <th>1.013958</th>\n",
       "    <th>0.875604</th>\n",
       "    <th>0.666667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>78</th>\n",
       "    <th>0.917848</th>\n",
       "    <th>0.861264</th>\n",
       "    <th>0.666667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>79</th>\n",
       "    <th>0.875393</th>\n",
       "    <th>0.847685</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>80</th>\n",
       "    <th>0.895439</th>\n",
       "    <th>0.971502</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>81</th>\n",
       "    <th>0.877830</th>\n",
       "    <th>0.855733</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>82</th>\n",
       "    <th>0.870198</th>\n",
       "    <th>0.906322</th>\n",
       "    <th>0.633333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>83</th>\n",
       "    <th>0.967336</th>\n",
       "    <th>0.906497</th>\n",
       "    <th>0.650000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>84</th>\n",
       "    <th>0.986497</th>\n",
       "    <th>0.987195</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>85</th>\n",
       "    <th>0.929289</th>\n",
       "    <th>0.904962</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>86</th>\n",
       "    <th>0.938599</th>\n",
       "    <th>0.811949</th>\n",
       "    <th>0.666667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>87</th>\n",
       "    <th>0.960672</th>\n",
       "    <th>0.775793</th>\n",
       "    <th>0.716667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>88</th>\n",
       "    <th>0.897664</th>\n",
       "    <th>0.776433</th>\n",
       "    <th>0.666667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>89</th>\n",
       "    <th>0.907984</th>\n",
       "    <th>0.852284</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>90</th>\n",
       "    <th>0.852352</th>\n",
       "    <th>0.789371</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>91</th>\n",
       "    <th>0.828371</th>\n",
       "    <th>0.870477</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>92</th>\n",
       "    <th>0.812102</th>\n",
       "    <th>0.840198</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>93</th>\n",
       "    <th>0.866907</th>\n",
       "    <th>0.789745</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>94</th>\n",
       "    <th>0.803867</th>\n",
       "    <th>0.828498</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>95</th>\n",
       "    <th>0.829514</th>\n",
       "    <th>0.794899</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>96</th>\n",
       "    <th>0.827412</th>\n",
       "    <th>0.815379</th>\n",
       "    <th>0.700000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>97</th>\n",
       "    <th>0.812185</th>\n",
       "    <th>0.778006</th>\n",
       "    <th>0.716667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>98</th>\n",
       "    <th>0.834942</th>\n",
       "    <th>0.802566</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>99</th>\n",
       "    <th>0.812494</th>\n",
       "    <th>0.814997</th>\n",
       "    <th>0.683333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>100</th>\n",
       "    <th>0.771604</th>\n",
       "    <th>0.857073</th>\n",
       "    <th>0.666667</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = trainNetwork(\"Worms\",lr=1e-2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:35 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>-4.347342</th>\n",
       "    <th>0.445190</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>-3.736607</th>\n",
       "    <th>0.411819</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>-3.558654</th>\n",
       "    <th>0.379419</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>-3.483701</th>\n",
       "    <th>0.351606</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>-3.455778</th>\n",
       "    <th>0.337507</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>-1.643021</th>\n",
       "    <th>0.338031</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>-0.661304</th>\n",
       "    <th>0.337008</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>-0.193588</th>\n",
       "    <th>0.339194</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>0.099940</th>\n",
       "    <th>0.340972</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>0.171600</th>\n",
       "    <th>0.333909</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>0.307155</th>\n",
       "    <th>0.362725</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>0.267742</th>\n",
       "    <th>0.382002</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>0.322468</th>\n",
       "    <th>0.421524</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>-1.346122</th>\n",
       "    <th>0.333671</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>-0.556964</th>\n",
       "    <th>0.327216</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>-1.860293</th>\n",
       "    <th>0.350831</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>-0.831481</th>\n",
       "    <th>0.381810</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>-2.017437</th>\n",
       "    <th>0.372594</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>-0.938705</th>\n",
       "    <th>0.562532</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>20</th>\n",
       "    <th>-2.118979</th>\n",
       "    <th>0.392646</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>21</th>\n",
       "    <th>-0.926058</th>\n",
       "    <th>0.385888</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>22</th>\n",
       "    <th>-0.318732</th>\n",
       "    <th>0.355633</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>23</th>\n",
       "    <th>0.003097</th>\n",
       "    <th>1.368310</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>24</th>\n",
       "    <th>-1.571072</th>\n",
       "    <th>0.515826</th>\n",
       "    <th>0.300000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>25</th>\n",
       "    <th>-0.721164</th>\n",
       "    <th>0.442058</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>26</th>\n",
       "    <th>-0.183517</th>\n",
       "    <th>0.701708</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>27</th>\n",
       "    <th>0.074180</th>\n",
       "    <th>0.371317</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>28</th>\n",
       "    <th>0.190623</th>\n",
       "    <th>0.443703</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>29</th>\n",
       "    <th>-1.533793</th>\n",
       "    <th>0.397362</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>30</th>\n",
       "    <th>-0.598887</th>\n",
       "    <th>0.579192</th>\n",
       "    <th>0.283333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>31</th>\n",
       "    <th>-0.111302</th>\n",
       "    <th>0.417758</th>\n",
       "    <th>0.383333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>32</th>\n",
       "    <th>0.121676</th>\n",
       "    <th>0.351905</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>33</th>\n",
       "    <th>0.240151</th>\n",
       "    <th>0.386255</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>34</th>\n",
       "    <th>0.314556</th>\n",
       "    <th>0.337116</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>35</th>\n",
       "    <th>0.363286</th>\n",
       "    <th>0.438416</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>36</th>\n",
       "    <th>0.327027</th>\n",
       "    <th>0.343511</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>37</th>\n",
       "    <th>0.345198</th>\n",
       "    <th>0.987325</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>38</th>\n",
       "    <th>0.384464</th>\n",
       "    <th>0.514152</th>\n",
       "    <th>0.333333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>39</th>\n",
       "    <th>0.462066</th>\n",
       "    <th>0.364631</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>40</th>\n",
       "    <th>0.458966</th>\n",
       "    <th>0.347085</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>41</th>\n",
       "    <th>0.410229</th>\n",
       "    <th>0.310056</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>42</th>\n",
       "    <th>0.385773</th>\n",
       "    <th>0.397065</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>43</th>\n",
       "    <th>0.412908</th>\n",
       "    <th>0.337033</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>44</th>\n",
       "    <th>0.320292</th>\n",
       "    <th>0.824042</th>\n",
       "    <th>0.233333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>45</th>\n",
       "    <th>0.159184</th>\n",
       "    <th>0.539520</th>\n",
       "    <th>0.350000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>46</th>\n",
       "    <th>-1.610795</th>\n",
       "    <th>0.438967</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>47</th>\n",
       "    <th>-2.487837</th>\n",
       "    <th>0.352797</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>48</th>\n",
       "    <th>-1.140981</th>\n",
       "    <th>0.468939</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>49</th>\n",
       "    <th>-0.431922</th>\n",
       "    <th>0.417790</th>\n",
       "    <th>0.416667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>50</th>\n",
       "    <th>-1.877508</th>\n",
       "    <th>0.701921</th>\n",
       "    <th>0.333333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>51</th>\n",
       "    <th>-0.805799</th>\n",
       "    <th>0.375699</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>52</th>\n",
       "    <th>-0.220637</th>\n",
       "    <th>0.334819</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>53</th>\n",
       "    <th>0.062855</th>\n",
       "    <th>0.527777</th>\n",
       "    <th>0.316667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>54</th>\n",
       "    <th>0.238301</th>\n",
       "    <th>0.396722</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>55</th>\n",
       "    <th>0.343630</th>\n",
       "    <th>0.317985</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>56</th>\n",
       "    <th>0.371981</th>\n",
       "    <th>0.908990</th>\n",
       "    <th>0.250000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>57</th>\n",
       "    <th>-1.439195</th>\n",
       "    <th>0.379508</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>58</th>\n",
       "    <th>-2.430872</th>\n",
       "    <th>0.405341</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>59</th>\n",
       "    <th>-1.166211</th>\n",
       "    <th>0.349951</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>60</th>\n",
       "    <th>-2.228200</th>\n",
       "    <th>0.340686</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>61</th>\n",
       "    <th>-1.070416</th>\n",
       "    <th>0.402688</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>62</th>\n",
       "    <th>-0.422184</th>\n",
       "    <th>0.357057</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>63</th>\n",
       "    <th>-0.045018</th>\n",
       "    <th>0.341351</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>64</th>\n",
       "    <th>-1.575618</th>\n",
       "    <th>0.360562</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>65</th>\n",
       "    <th>-0.683042</th>\n",
       "    <th>0.375977</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>66</th>\n",
       "    <th>-0.187051</th>\n",
       "    <th>0.340642</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>67</th>\n",
       "    <th>-1.749934</th>\n",
       "    <th>0.437363</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>68</th>\n",
       "    <th>-0.752392</th>\n",
       "    <th>0.356093</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>69</th>\n",
       "    <th>-2.070403</th>\n",
       "    <th>0.358591</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>70</th>\n",
       "    <th>-2.768805</th>\n",
       "    <th>0.385902</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>71</th>\n",
       "    <th>-1.346030</th>\n",
       "    <th>0.412008</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>72</th>\n",
       "    <th>-2.389117</th>\n",
       "    <th>0.381857</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>73</th>\n",
       "    <th>-1.130042</th>\n",
       "    <th>0.375097</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>74</th>\n",
       "    <th>-2.313262</th>\n",
       "    <th>0.404908</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>75</th>\n",
       "    <th>-2.901753</th>\n",
       "    <th>0.350766</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>76</th>\n",
       "    <th>-3.165075</th>\n",
       "    <th>0.386667</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>77</th>\n",
       "    <th>-3.382471</th>\n",
       "    <th>0.424196</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>78</th>\n",
       "    <th>-3.509089</th>\n",
       "    <th>0.411398</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>79</th>\n",
       "    <th>-3.547938</th>\n",
       "    <th>0.410659</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>80</th>\n",
       "    <th>-3.630021</th>\n",
       "    <th>0.362847</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>81</th>\n",
       "    <th>-1.801700</th>\n",
       "    <th>0.424970</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>82</th>\n",
       "    <th>-2.584016</th>\n",
       "    <th>0.452899</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>83</th>\n",
       "    <th>-1.265378</th>\n",
       "    <th>0.466191</th>\n",
       "    <th>0.433333</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>84</th>\n",
       "    <th>-0.508414</th>\n",
       "    <th>0.401867</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>85</th>\n",
       "    <th>-0.133157</th>\n",
       "    <th>0.395072</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>86</th>\n",
       "    <th>-1.697159</th>\n",
       "    <th>0.419596</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>87</th>\n",
       "    <th>-0.765211</th>\n",
       "    <th>0.421085</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>88</th>\n",
       "    <th>-0.297720</th>\n",
       "    <th>0.424526</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>89</th>\n",
       "    <th>0.025865</th>\n",
       "    <th>0.420502</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>90</th>\n",
       "    <th>0.206796</th>\n",
       "    <th>0.414281</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>91</th>\n",
       "    <th>0.303267</th>\n",
       "    <th>0.436358</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>92</th>\n",
       "    <th>-1.460511</th>\n",
       "    <th>0.450463</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>93</th>\n",
       "    <th>-4.237714</th>\n",
       "    <th>0.447838</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>94</th>\n",
       "    <th>-2.113509</th>\n",
       "    <th>0.430008</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>95</th>\n",
       "    <th>-0.993820</th>\n",
       "    <th>0.430656</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>96</th>\n",
       "    <th>-2.180183</th>\n",
       "    <th>0.418926</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>97</th>\n",
       "    <th>-1.074332</th>\n",
       "    <th>0.435356</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>98</th>\n",
       "    <th>-2.227536</th>\n",
       "    <th>0.443871</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>99</th>\n",
       "    <th>-2.905497</th>\n",
       "    <th>0.462517</th>\n",
       "    <th>0.450000</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>100</th>\n",
       "    <th>-1.384670</th>\n",
       "    <th>0.428797</th>\n",
       "    <th>0.466667</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.conv.weight',\n",
       "              tensor([[[ 0.1073, -0.0872, -0.0879,  ..., -0.1043, -0.0412, -0.0535]],\n",
       "              \n",
       "                      [[-0.2463, -0.0052,  0.2100,  ...,  0.0963, -0.1249, -0.1503]],\n",
       "              \n",
       "                      [[ 0.2216,  0.1549,  0.1031,  ..., -0.0185,  0.0382, -0.1180]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.5980,  0.6302,  0.6335,  ...,  0.1199,  0.0938,  0.0895]],\n",
       "              \n",
       "                      [[-0.0889,  0.2088, -0.0529,  ..., -0.1337, -0.1374,  0.0765]],\n",
       "              \n",
       "                      [[ 0.5792,  0.6602,  0.2864,  ...,  0.0325, -0.3880, -0.7142]]],\n",
       "                     device='cuda:0')),\n",
       "             ('0.conv.bias',\n",
       "              tensor([-0.3000, -0.1017, -0.3986, -0.0902, -0.3298, -0.1704,  0.4436, -0.2207,\n",
       "                       0.2127,  0.3475,  0.1311, -0.2273,  0.4033, -0.0943,  0.0187, -0.1235,\n",
       "                      -0.2214, -0.1711, -0.5492, -0.6197,  0.8057, -0.4565, -0.3390,  0.9701,\n",
       "                      -0.1627, -0.0567, -0.2202,  0.0793, -0.1322, -0.0111,  0.3882, -0.4619,\n",
       "                      -0.4524,  0.2898, -0.0689,  0.2401, -0.1528, -0.3703, -0.4816,  0.5172,\n",
       "                      -0.2445,  0.3466, -0.1152,  0.1164, -0.1381,  0.0221,  0.5503, -0.1951,\n",
       "                      -0.4741, -0.2319, -0.0654, -0.2375, -0.0471, -0.3089, -0.1512,  0.3562,\n",
       "                      -0.2007,  0.2165, -0.4306,  0.1676, -0.0684,  0.0947, -0.3040,  0.4112,\n",
       "                      -0.3161,  0.0535,  0.2044,  0.0753, -0.3149,  0.2895, -0.0726, -0.1258,\n",
       "                      -0.4573,  0.6328,  0.1752, -0.1562, -0.2008, -0.1534, -0.5117,  0.5828,\n",
       "                      -0.1911,  0.5998,  0.1671, -0.1936, -0.3798,  0.4427, -0.5061, -0.0967,\n",
       "                      -0.0128, -0.3238, -0.1222, -0.1694,  0.3115, -0.1015,  0.2092, -0.1609,\n",
       "                       0.5772, -0.5558, -0.4131,  0.2290, -0.1267,  0.1567, -0.3689,  0.2832,\n",
       "                       0.3981, -0.3017,  0.0256, -0.2552,  0.3432, -0.0876, -0.0919, -0.2385,\n",
       "                      -0.2349, -0.2077,  0.2460,  0.3112,  0.1739, -0.0766,  0.4946,  0.4711,\n",
       "                       0.2478, -0.1071, -0.1573, -0.5143, -0.4819, -0.1075, -0.2709,  0.2934],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bn.weight',\n",
       "              tensor([ 0.2893,  0.0178,  0.1949,  0.5913,  0.6192,  0.5395,  0.9764,  0.2910,\n",
       "                       0.2822,  0.7352,  0.7422,  0.7044,  0.4465,  0.3874,  0.3065,  0.4373,\n",
       "                      -0.3147,  0.8924,  0.2265, -0.2603, -0.1095,  0.2590,  0.5417,  0.5008,\n",
       "                       0.6185,  0.1065,  0.2511,  0.5545,  0.3355,  0.4117,  0.8053,  0.2119,\n",
       "                       0.4325,  0.8231,  0.1569, -0.0285,  0.1248,  0.1743,  0.3465, -0.0332,\n",
       "                       0.6482,  1.0004,  0.0492,  0.1553,  0.6578,  0.2871,  0.4281,  0.4921,\n",
       "                       0.5491,  0.2995,  0.3046,  0.4832,  0.0911,  0.0756,  0.6348,  0.9424,\n",
       "                       0.5980,  0.3334,  0.4684, -0.0958,  0.3778,  0.4873,  0.4086,  1.1633,\n",
       "                       0.4832, -0.1396,  0.3292,  0.4179,  0.3301,  0.1622,  0.1619,  0.3740,\n",
       "                       0.5625,  0.1497,  0.5109,  1.6409,  0.1571,  0.2099,  0.1066,  1.1207,\n",
       "                       0.5337,  1.0062,  0.2650,  0.7367,  0.3655,  0.8141,  0.4708, -0.1954,\n",
       "                       0.7843,  0.3253,  0.3835,  0.2566,  0.3527,  0.2281,  1.0446,  0.0840,\n",
       "                      -0.0173,  0.5666,  0.3460,  0.3385,  0.5348,  0.1375,  0.2393,  0.5203,\n",
       "                       0.1923,  0.3201,  0.6838,  0.5920,  0.7124,  0.2556,  0.5721,  0.5969,\n",
       "                       0.8218,  0.3731,  0.9089,  0.9523,  0.3584,  0.0313,  1.1349,  1.0248,\n",
       "                       0.0573,  1.1530,  0.4816, -0.3567,  0.2280,  0.4332,  0.4651,  0.3429],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bn.bias',\n",
       "              tensor([ 0.0549,  0.0108,  0.1095,  0.2626, -0.1204,  0.2700,  0.5106,  0.0985,\n",
       "                       0.2024,  0.3987,  0.1152,  0.3173,  0.2884,  0.6501,  0.6833,  0.3382,\n",
       "                      -0.1254, -0.0572,  0.1915,  0.6801, -0.1130,  0.3308, -0.0399,  0.0288,\n",
       "                       0.0614,  0.3277, -0.3640,  0.2622, -0.2512,  0.2552, -0.0420,  0.0540,\n",
       "                      -0.0458, -0.3003,  0.0662, -0.4449, -0.0016,  0.1521, -0.0899, -0.1404,\n",
       "                      -0.0588,  0.3257, -0.8003,  0.6126,  0.2253,  0.1976,  0.3456,  0.0564,\n",
       "                       0.3933,  0.1622, -0.0028, -0.0629,  0.4597,  0.2159,  0.2753,  0.6357,\n",
       "                      -0.2458,  0.7231, -0.3323,  0.2006,  0.1944,  0.5242,  0.1118,  0.2362,\n",
       "                       0.0776, -0.1952,  0.1114,  0.5961,  0.0370,  0.0519,  0.1160,  0.2783,\n",
       "                       0.0686, -0.1215,  0.0975,  0.0069, -0.1385,  0.5631,  0.2998,  0.4415,\n",
       "                       0.4006,  0.3480, -0.3055,  0.2095,  0.1306,  0.2847,  0.1142, -0.0945,\n",
       "                      -0.0327,  0.3459,  0.2828, -0.1446,  0.2258, -0.2423,  0.0843,  0.6252,\n",
       "                       0.1281, -0.1864, -0.1327,  0.4881,  0.1892, -0.5333,  0.1602,  0.1280,\n",
       "                       0.2143, -0.1211,  0.5582, -0.0079,  0.2098, -0.1247,  0.3293, -0.3617,\n",
       "                       0.5473,  0.3725,  0.2642,  0.3138,  0.0621,  0.2220,  0.0947,  0.4737,\n",
       "                       0.0089,  0.2511,  0.4972,  0.1586, -0.0329,  0.2626,  0.3125,  0.3931],\n",
       "                     device='cuda:0')),\n",
       "             ('0.bn.running_mean',\n",
       "              tensor([5.6052e-45, 5.6052e-45, 1.5341e-01, 6.1014e-02, 5.6052e-45, 1.5017e+00,\n",
       "                      4.5349e-01, 8.0154e-01, 8.2358e-01, 3.6642e-01, 4.4439e-01, 5.6052e-45,\n",
       "                      4.0106e-01, 7.8227e-01, 1.6618e+00, 5.6052e-45, 1.7078e+00, 4.5711e-02,\n",
       "                      5.6052e-45, 5.6052e-45, 8.0281e-01, 5.6052e-45, 5.6052e-45, 1.2898e+00,\n",
       "                      0.0000e+00, 3.8054e-01, 5.6052e-45, 2.2701e-01, 5.6052e-45, 8.2809e-01,\n",
       "                      4.6035e-01, 5.6052e-45, 5.6052e-45, 5.4408e-01, 6.4362e-01, 1.1377e+00,\n",
       "                      1.6924e+00, 5.6052e-45, 5.6052e-45, 1.0267e+00, 5.6052e-45, 3.6390e-01,\n",
       "                      5.6052e-45, 1.2070e+00, 6.0292e-01, 1.0806e+00, 7.8146e-01, 5.6052e-45,\n",
       "                      5.8245e-02, 5.6052e-45, 4.4146e-01, 5.6052e-45, 1.3641e-01, 5.6052e-45,\n",
       "                      0.0000e+00, 3.7057e-01, 5.6052e-45, 7.3906e-01, 5.6052e-45, 1.9462e+00,\n",
       "                      9.1278e-01, 1.5141e+00, 5.6052e-45, 7.8870e-01, 5.6052e-45, 1.7428e+00,\n",
       "                      1.8201e+00, 1.6226e+00, 5.6052e-45, 4.9445e-01, 5.6052e-45, 1.4781e+00,\n",
       "                      5.6052e-45, 9.5656e-01, 1.5733e+00, 1.2191e-02, 5.6052e-45, 9.8464e-01,\n",
       "                      5.6052e-45, 5.9802e-01, 5.6052e-45, 6.1577e-01, 1.2257e+00, 0.0000e+00,\n",
       "                      5.6052e-45, 4.5750e-01, 5.6052e-45, 1.4695e+00, 2.4395e-01, 5.6052e-45,\n",
       "                      6.6908e-02, 5.8053e-01, 3.2255e-01, 5.6052e-45, 2.1903e-01, 2.6492e-01,\n",
       "                      8.2643e-01, 5.6052e-45, 5.6052e-45, 4.1173e-01, 9.2480e-02, 1.0655e+00,\n",
       "                      5.4348e-01, 9.7612e-01, 8.7486e-01, 5.6052e-45, 5.3305e-01, 5.6052e-45,\n",
       "                      5.8888e-01, 4.3217e-01, 1.2129e+00, 5.6052e-45, 3.0446e-01, 5.6052e-45,\n",
       "                      3.9386e-01, 3.2322e-01, 7.4696e-01, 1.4333e+00, 5.0803e-01, 4.8894e-01,\n",
       "                      3.5804e-01, 1.0775e-01, 1.8891e-01, 5.6052e-45, 5.6052e-45, 1.2565e+00,\n",
       "                      5.6052e-45, 6.0132e-01], device='cuda:0')),\n",
       "             ('0.bn.running_var',\n",
       "              tensor([5.6052e-45, 5.6052e-45, 6.1329e-02, 8.8152e-03, 5.6052e-45, 3.6779e+00,\n",
       "                      1.2679e-01, 1.1013e+00, 9.6761e-01, 9.7244e-02, 2.8119e-01, 5.6052e-45,\n",
       "                      6.3947e-02, 1.0031e+00, 4.3172e+00, 5.6052e-45, 4.7533e+00, 7.1272e-03,\n",
       "                      5.6052e-45, 5.6052e-45, 2.6621e-02, 5.6052e-45, 5.6052e-45, 1.7001e+00,\n",
       "                      5.6052e-45, 2.4236e-01, 5.6052e-45, 6.5733e-02, 5.6052e-45, 1.0859e+00,\n",
       "                      1.7627e-01, 5.6052e-45, 5.6052e-45, 3.6673e-01, 6.7116e-01, 1.8510e+00,\n",
       "                      4.6063e+00, 5.6052e-45, 5.6052e-45, 1.2868e+00, 5.6052e-45, 9.4263e-02,\n",
       "                      5.6052e-45, 2.2098e+00, 6.2028e-01, 1.8056e+00, 6.5431e-01, 5.6052e-45,\n",
       "                      1.5233e-02, 5.6052e-45, 3.2280e-01, 5.6052e-45, 3.3320e-02, 5.6052e-45,\n",
       "                      5.6052e-45, 6.8973e-02, 5.6052e-45, 7.6911e-01, 5.6052e-45, 5.7060e+00,\n",
       "                      1.3334e+00, 3.4883e+00, 5.6052e-45, 7.7067e-01, 5.6052e-45, 4.7185e+00,\n",
       "                      4.9389e+00, 4.0653e+00, 5.6052e-45, 2.8095e-01, 5.6052e-45, 3.5076e+00,\n",
       "                      5.6052e-45, 9.8714e-01, 3.6911e+00, 5.7316e-04, 5.6052e-45, 1.6028e+00,\n",
       "                      5.6052e-45, 2.2717e-01, 5.6052e-45, 1.7850e-01, 2.2176e+00, 5.6052e-45,\n",
       "                      5.6052e-45, 1.0144e-01, 5.6052e-45, 3.4665e+00, 9.6394e-02, 5.6052e-45,\n",
       "                      1.1264e-02, 5.8151e-01, 5.1104e-02, 5.6052e-45, 3.7286e-02, 1.3206e-01,\n",
       "                      7.3654e-01, 5.6052e-45, 5.6052e-45, 1.9909e-01, 2.0463e-02, 1.6682e+00,\n",
       "                      5.6239e-01, 1.3130e+00, 9.8635e-01, 5.6052e-45, 4.4313e-01, 5.6052e-45,\n",
       "                      3.9954e-01, 3.1712e-01, 2.3738e+00, 5.6052e-45, 1.8431e-01, 5.6052e-45,\n",
       "                      1.7159e-01, 7.0826e-02, 8.0456e-01, 3.2847e+00, 1.6458e-01, 1.6232e-01,\n",
       "                      1.3269e-01, 2.5330e-02, 7.3609e-02, 5.6052e-45, 5.6052e-45, 2.5518e+00,\n",
       "                      5.6052e-45, 4.6282e-01], device='cuda:0')),\n",
       "             ('0.bn.num_batches_tracked', tensor(6000, device='cuda:0')),\n",
       "             ('1.conv.weight',\n",
       "              tensor([[[-0.1965,  0.2589,  0.1078,  0.0745,  0.2593],\n",
       "                       [ 0.4332, -0.1053,  0.1451, -0.0378,  0.2223],\n",
       "                       [ 0.1823,  0.1792,  0.0473,  0.3972, -0.0824],\n",
       "                       ...,\n",
       "                       [-0.0580,  0.2504, -0.5668, -0.0395, -0.0271],\n",
       "                       [-0.3873, -0.1963, -0.2638, -0.3676,  0.1112],\n",
       "                       [-0.2536, -0.5460, -0.3032, -0.5131, -0.3591]],\n",
       "              \n",
       "                      [[ 0.0762, -0.5231, -0.0964, -0.1803,  0.0301],\n",
       "                       [ 0.0361, -0.3310, -0.4896,  0.4471,  0.0040],\n",
       "                       [ 0.2693, -0.6719, -0.3700,  0.1041, -0.0706],\n",
       "                       ...,\n",
       "                       [-0.7703, -0.7269, -0.7392, -0.9028, -0.9085],\n",
       "                       [-0.2625, -0.3588, -0.6350,  0.0255, -0.2855],\n",
       "                       [ 0.0984,  0.7005,  0.1842,  0.3459,  0.6241]],\n",
       "              \n",
       "                      [[ 0.3140,  0.3850,  0.0365,  0.1943,  0.3441],\n",
       "                       [ 0.3899,  0.4153,  0.6124,  0.2271,  0.2339],\n",
       "                       [ 1.1522,  0.2324,  0.1678,  0.4592,  0.6823],\n",
       "                       ...,\n",
       "                       [ 0.2191,  0.0309,  0.3757, -0.0761,  0.4445],\n",
       "                       [-0.1075, -0.1004, -0.4016, -0.3227,  0.0480],\n",
       "                       [-0.2904, -0.3378, -0.3901, -0.2652, -0.3205]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.1888,  0.3674, -0.3162, -0.1221,  0.6938],\n",
       "                       [ 0.3190,  0.0517, -0.3778,  0.0361,  0.5312],\n",
       "                       [ 0.5757,  0.4511,  0.2283,  0.2159,  0.4361],\n",
       "                       ...,\n",
       "                       [ 0.2418,  0.3805,  0.4856,  0.4703,  0.3270],\n",
       "                       [ 0.5741,  0.1132,  0.1002, -0.0143,  0.6433],\n",
       "                       [-0.3767, -0.0347, -0.5197, -0.3369,  0.1951]],\n",
       "              \n",
       "                      [[ 0.0628, -0.6500, -0.6948, -0.3672, -0.5954],\n",
       "                       [-0.3350, -0.4257,  0.1240,  0.3693,  0.0998],\n",
       "                       [ 0.3389,  0.1075,  0.0270,  0.0282,  0.2382],\n",
       "                       ...,\n",
       "                       [ 0.1354, -0.0369, -0.2221,  0.5137,  0.1954],\n",
       "                       [-0.2003, -0.7283,  0.4234, -0.6145, -0.2398],\n",
       "                       [ 0.1550,  0.0034,  0.1053, -0.1917,  0.2039]],\n",
       "              \n",
       "                      [[-0.1232, -0.1771, -0.0104, -0.0513, -0.3163],\n",
       "                       [-0.0834, -0.1480, -0.3615, -0.3430,  0.1282],\n",
       "                       [-0.0777,  0.1653, -0.4579, -0.4128, -0.4695],\n",
       "                       ...,\n",
       "                       [-0.6807, -0.2506, -0.4088, -0.1907, -0.3444],\n",
       "                       [-0.1444, -0.0633, -0.3509, -0.5106,  0.1309],\n",
       "                       [-0.1096, -0.2877, -0.0755, -0.1049, -0.3738]]], device='cuda:0')),\n",
       "             ('1.conv.bias',\n",
       "              tensor([-0.2988,  0.4014, -0.8954, -0.5406, -0.3042, -0.4399, -0.1187, -0.4832,\n",
       "                       0.4465, -0.4049, -0.2150,  0.3759, -0.6502, -0.1213,  0.2260,  0.2786,\n",
       "                       0.5145, -0.2725, -0.1713, -0.2398, -0.5050, -0.1065,  0.0305, -0.1795,\n",
       "                      -0.3568, -0.6827, -0.4101,  0.4721,  0.1430,  0.2070, -0.2562, -0.0617,\n",
       "                      -0.2874,  1.0159, -0.1183, -0.2317, -0.1140,  0.1902,  0.1716, -0.2247,\n",
       "                       0.0569, -0.5524, -0.5578,  0.6134, -0.0655, -0.1900,  0.1006,  0.1555,\n",
       "                      -0.3624,  0.0369, -0.5105,  0.5461, -0.2546,  0.0891, -0.7713, -0.1777,\n",
       "                      -0.6983, -0.1049,  0.1056, -0.0675,  0.1929,  0.7476, -0.5150, -0.0485,\n",
       "                      -0.3005, -0.0547, -0.1707, -0.4541, -0.3792, -0.1856, -0.2193, -0.4459,\n",
       "                       0.5598, -0.8812, -0.6862, -0.0939, -0.1071, -1.1324, -0.0141, -0.2559,\n",
       "                      -0.1181, -0.6307, -0.2839,  0.5829,  0.5834, -0.2914,  0.6635,  0.0203,\n",
       "                      -0.3403, -0.5372,  0.0173, -0.3452, -0.6370, -0.1010, -0.1208, -0.6224,\n",
       "                      -0.7338,  0.5730, -0.4082, -0.1839, -0.2200,  0.6871, -0.2441, -0.7627,\n",
       "                      -0.5082,  0.0489, -0.3788, -0.4021,  0.2464, -0.3640, -0.2408, -0.1492,\n",
       "                      -0.1308, -0.5574, -0.1927, -0.1840, -0.4863, -0.3893, -0.1722, -0.3205,\n",
       "                      -0.4705, -0.6202, -0.4497,  0.1778,  0.3809, -0.3503, -0.5570,  0.2150,\n",
       "                      -0.0611, -0.1391,  0.0169, -0.2350,  0.1017, -0.1828, -0.4509, -0.4532,\n",
       "                      -0.4537, -0.0973, -0.5021,  0.3539,  0.1485,  0.6995, -0.1002,  0.0617,\n",
       "                      -0.3451, -0.1281, -0.3133, -0.0899,  0.0481, -0.6346, -0.1783, -0.2843,\n",
       "                      -0.1989, -0.1984, -0.4536, -0.2303, -0.0767, -0.3059,  0.2097, -0.1632,\n",
       "                      -0.2880,  0.1093, -0.4156, -0.0780,  0.0601, -0.7330, -0.5569,  0.3685,\n",
       "                      -0.3009, -0.4533, -0.5490,  0.1500, -0.0239,  0.1492,  0.0552, -0.5925,\n",
       "                       0.0828,  0.1995, -0.0183,  0.0281,  0.0996,  0.0871,  0.1698,  0.0108,\n",
       "                      -0.0024,  0.3432, -0.2594, -0.1459, -0.2537, -0.0963,  0.0081,  0.2323,\n",
       "                       1.0044,  0.8891, -0.4894, -0.4567, -0.3185,  0.1546,  0.1897, -0.3162,\n",
       "                      -0.6767, -0.5966, -0.3165, -0.4165, -0.1074, -0.3599, -0.0273, -0.4009,\n",
       "                       0.1889, -0.5501, -0.5098,  0.4293, -0.4846, -0.3225, -0.8313,  0.2405,\n",
       "                      -0.3035, -0.8743,  0.0174, -0.2370,  0.7114, -0.7879, -0.2668, -0.4215,\n",
       "                      -0.4680,  0.4601, -0.1509, -0.1095, -0.3779, -0.2527, -0.5578, -0.0754,\n",
       "                      -0.1755, -0.6965,  0.5132,  0.1895, -0.1410,  0.2468, -0.0731, -0.1291,\n",
       "                      -0.4739, -0.3309, -0.2790, -0.2562,  0.2266, -0.8865, -0.1756, -0.2180,\n",
       "                      -0.2928, -0.6956, -0.5667,  0.1246, -0.1288, -0.1554,  0.4543,  0.0476],\n",
       "                     device='cuda:0')),\n",
       "             ('1.bn.weight',\n",
       "              tensor([ 0.3162,  0.6795,  0.8684, -0.2827,  0.7511,  0.0318,  0.6158,  0.0836,\n",
       "                       0.4083,  0.3263,  0.8141,  0.3635,  0.6426,  0.2464, -0.1180,  0.4024,\n",
       "                       0.3944, -0.0366,  0.4409,  0.1328, -0.0881,  0.4875,  1.0041,  0.4823,\n",
       "                       0.8221,  0.2187,  0.0254,  0.3930, -0.1421,  0.1194,  1.0850,  0.5295,\n",
       "                       1.1931,  0.7323,  0.2433,  0.3575,  1.2139, -0.3411,  0.3292,  0.2134,\n",
       "                      -0.0999,  0.7580, -0.0143,  0.7823,  0.4934,  0.3092,  0.0059,  0.3727,\n",
       "                       1.3279, -0.7610,  0.5318,  0.1040,  0.7322, -0.6797,  0.4608,  0.1464,\n",
       "                      -0.2987, -0.2232, -0.0136,  0.9336,  0.3579, -0.0067, -0.1177,  0.1809,\n",
       "                       0.1261,  0.3448, -0.0434,  0.4620,  0.9985,  1.1530,  0.6776,  0.7745,\n",
       "                       0.1482,  0.1970,  0.0149, -0.0322,  0.8097,  1.1153, -0.1045,  0.7229,\n",
       "                       0.9059,  0.0838, -0.8109,  0.3492, -0.0259,  1.3690,  0.0813,  0.0992,\n",
       "                       0.2670,  0.1260,  0.9684,  0.4761,  0.1782,  0.1204,  0.0596, -0.0852,\n",
       "                       0.3175,  0.2107,  0.4376,  0.6605,  0.3021, -0.4092,  0.2330,  0.5532,\n",
       "                      -0.4224,  0.0669,  0.0092,  0.6612,  0.2385,  0.8615, -0.3303,  0.7390,\n",
       "                      -0.0713,  0.4913,  0.1972,  0.3586,  0.4832,  1.2949,  0.7615, -0.2510,\n",
       "                       0.4868,  0.3373,  0.0946,  0.1493,  0.3474,  0.1070,  0.3325, -0.3042,\n",
       "                       0.3723,  0.1381,  0.5174,  0.8429, -0.5321,  0.0197,  0.8715,  0.3854,\n",
       "                       0.5812,  1.4778,  0.4035,  0.0731,  0.5480,  0.0251,  0.9470,  0.2689,\n",
       "                       0.7852, -0.3117,  0.3983,  0.1055,  0.7356,  0.4758,  0.2401,  0.2272,\n",
       "                       0.6301,  0.3754,  0.1060,  0.4956,  1.0239,  0.8053, -0.2090,  0.5136,\n",
       "                       1.2670,  0.1805,  1.3628, -0.1064,  0.3436,  0.4366,  0.4381,  0.1403,\n",
       "                       0.1905, -0.1620, -0.1397, -0.1912,  0.3954,  0.2183,  0.0855,  0.0745,\n",
       "                       0.2535,  0.3880,  0.6183,  0.4538,  0.1927,  0.8524,  0.5268,  0.2072,\n",
       "                       0.2973, -0.1441,  0.5725,  0.1133,  0.3753,  0.0808,  0.3082,  0.3274,\n",
       "                      -0.3584,  0.6444,  0.5853, -0.0264,  0.5779,  0.6344, -0.2416, -0.6599,\n",
       "                       0.3769,  0.2943,  0.5157,  0.3768, -0.0469,  0.9569,  0.1401,  0.5877,\n",
       "                       0.6316,  0.8579,  0.2624,  0.1204,  0.5557,  0.6432,  0.7989,  0.2394,\n",
       "                       0.4326,  0.6744, -0.1353,  0.6821, -0.1459,  0.4329,  0.2081,  0.1128,\n",
       "                       0.3733,  0.3338,  0.0674,  0.0946,  0.0561,  0.4240,  0.2519, -0.7728,\n",
       "                      -0.3250,  0.2489,  0.6487, -0.0577, -0.3096,  0.4183,  0.4013,  0.5378,\n",
       "                       0.4934,  0.3479,  0.2562,  0.4859,  0.3748,  0.5724,  1.0444,  0.8989,\n",
       "                       0.5738,  0.3695,  0.3250,  0.2311,  0.5673,  0.4749,  0.2347,  0.4513],\n",
       "                     device='cuda:0')),\n",
       "             ('1.bn.bias',\n",
       "              tensor([ 1.3329e-01, -5.3939e-02,  2.4875e-01,  6.7043e-01,  1.6978e-01,\n",
       "                      -8.0596e-02,  3.9326e-01, -6.4553e-01, -4.0195e-01,  2.1250e-01,\n",
       "                      -1.0660e-01, -3.3558e-01,  5.1119e-01,  2.0352e-01, -8.8935e-02,\n",
       "                      -6.5378e-01,  4.6245e-01, -1.1294e-01, -3.9311e-02,  3.8778e-01,\n",
       "                       3.0943e-01, -2.8795e-01,  4.5002e-01,  3.9677e-02, -2.9628e-01,\n",
       "                       2.5037e-01,  5.1614e-02,  1.7865e-01, -2.4719e-01, -8.7035e-01,\n",
       "                       7.5219e-01,  1.4697e-01,  4.2684e-01, -4.2666e-01,  3.8866e-02,\n",
       "                      -2.0084e-01,  1.8047e-02,  3.8866e-01,  6.8105e-01, -1.0183e-01,\n",
       "                       4.6315e-01, -1.5321e-01, -4.4491e-01, -1.0098e-01,  4.5057e-01,\n",
       "                       1.8363e-01, -5.4016e-01, -2.5393e-01,  5.7929e-01, -5.2665e-01,\n",
       "                       3.0210e-01, -1.7488e-01, -2.1582e-01,  1.7256e-01,  2.5852e-01,\n",
       "                       3.6182e-01,  1.9736e-01,  4.1516e-01, -1.2722e-01,  6.2606e-02,\n",
       "                       4.1067e-01,  3.1581e-01,  1.9903e-02, -1.3125e-01,  2.7266e-01,\n",
       "                      -2.3369e-01,  5.2547e-01, -1.4605e-01,  6.5368e-01,  1.1002e-01,\n",
       "                      -1.4899e-01,  3.0667e-02,  1.8746e-01, -1.0282e-02,  1.5678e-01,\n",
       "                       2.5902e-01,  1.4711e-01,  3.5447e-01,  3.3897e-01,  6.2360e-01,\n",
       "                       3.4779e-01, -6.3137e-01,  1.3694e-01, -3.5741e-01,  2.5887e-01,\n",
       "                       1.4786e-01,  7.7830e-02,  7.1315e-01,  1.9479e-01, -6.2531e-01,\n",
       "                       1.1173e+00, -3.3432e-01,  1.3069e-01,  8.4092e-02,  4.5845e-01,\n",
       "                      -2.9079e-02, -5.9239e-01, -5.7654e-01,  2.8233e-01, -2.4323e-01,\n",
       "                      -1.6744e-01,  6.0799e-01,  1.5441e-01,  4.0321e-02,  9.6415e-01,\n",
       "                       4.3738e-01,  8.7800e-01,  1.3030e-02, -8.9255e-04, -2.8570e-01,\n",
       "                       4.7065e-01,  3.9367e-01,  2.1596e-01,  1.5672e-01,  5.1148e-01,\n",
       "                      -3.5146e-01, -4.8132e-01,  5.2493e-01,  6.7294e-02, -1.7751e-01,\n",
       "                       7.1127e-03, -9.5797e-02,  4.5088e-01,  4.9424e-01, -3.8934e-01,\n",
       "                       3.9054e-01,  6.4096e-02,  1.8649e-01, -1.2180e-01,  2.9558e-01,\n",
       "                       4.7441e-03,  1.5189e-02,  2.4173e-01,  2.5489e-01,  5.0652e-01,\n",
       "                       1.9223e-01,  8.6825e-01, -2.4698e-01,  7.7213e-01,  2.7757e-01,\n",
       "                       3.3962e-01, -2.3464e-01, -3.8315e-03,  4.5279e-01, -1.1642e-01,\n",
       "                       3.4543e-01, -2.8168e-02, -7.2525e-01,  1.5605e-01,  3.2529e-01,\n",
       "                      -3.7358e-02,  7.5604e-01,  4.0837e-01,  2.1506e-01, -3.6357e-01,\n",
       "                      -7.7282e-02, -5.0869e-01, -1.7273e-01,  5.0010e-02,  2.7047e-01,\n",
       "                       2.3787e-01,  6.2264e-02,  3.0753e-01,  9.1648e-02,  6.5960e-01,\n",
       "                       3.7362e-01,  3.7751e-01, -2.7191e-01,  4.9862e-01,  9.0747e-01,\n",
       "                      -3.1272e-01,  7.7221e-01,  2.9682e-01,  2.2496e-01,  3.0517e-01,\n",
       "                      -2.0884e-01,  2.9784e-01, -3.2274e-01,  1.2232e-01,  1.3659e-01,\n",
       "                       2.3249e-02,  2.0575e-01,  3.0648e-01,  6.3047e-01,  6.4756e-01,\n",
       "                      -3.6720e-01,  5.8855e-01, -5.3147e-01,  5.5186e-01, -2.0161e-02,\n",
       "                       9.1804e-01, -3.1166e-02,  3.0499e-01, -2.9504e-01,  9.5317e-01,\n",
       "                       2.2770e-02,  4.1278e-01,  3.8707e-02,  7.1005e-01, -1.8781e-01,\n",
       "                      -2.0499e-01,  1.3971e-01,  4.1369e-01,  1.1396e-01,  3.1343e-01,\n",
       "                      -4.7798e-02,  1.6926e-01,  2.6122e-02, -2.6799e-01,  1.6855e-01,\n",
       "                       1.5766e-01,  6.5077e-01,  5.5595e-01,  1.0831e-01,  3.6856e-01,\n",
       "                      -5.7565e-02, -4.7618e-02,  2.0427e-01,  3.0351e-01, -1.5807e-01,\n",
       "                       3.3169e-01, -2.6381e-01,  3.0096e-01,  6.3914e-02,  4.5182e-01,\n",
       "                      -4.8824e-01,  3.6789e-01,  2.6806e-01,  4.6425e-02, -1.4696e-01,\n",
       "                       1.4697e-01, -2.6314e-01,  4.4846e-01, -2.7384e-01, -1.7277e-01,\n",
       "                       3.2558e-01,  7.9275e-02,  4.9512e-02,  5.6920e-01,  2.6940e-01,\n",
       "                      -2.4045e-01,  6.7076e-01,  2.6297e-02,  2.3963e-01,  8.3077e-01,\n",
       "                      -1.0559e-01,  3.8748e-01,  5.6903e-01,  7.7101e-02,  8.3639e-02,\n",
       "                      -4.0708e-01, -3.0347e-01,  8.3601e-02,  4.1938e-01, -1.4569e-01,\n",
       "                      -5.1965e-02], device='cuda:0')),\n",
       "             ('1.bn.running_mean',\n",
       "              tensor([2.3985e-04, 2.6190e+00, 2.3532e-03, 1.7028e-01, 1.8004e+00, 2.6396e-03,\n",
       "                      1.1145e+00, 1.5728e+00, 2.4839e+00, 1.0976e+01, 1.4682e+01, 6.2386e+00,\n",
       "                      5.9619e-05, 5.3644e+00, 3.0339e+00, 5.8638e+00, 5.0210e+00, 5.1191e+00,\n",
       "                      1.6277e+01, 7.6952e+00, 4.7683e+00, 2.2193e+01, 2.0769e+00, 1.8345e+01,\n",
       "                      7.2415e+00, 5.6052e-45, 2.1672e+00, 1.8882e+01, 2.7211e+00, 8.3677e+00,\n",
       "                      1.2218e+01, 1.1962e+01, 1.1861e+01, 8.6328e+00, 8.6665e+00, 3.8165e+00,\n",
       "                      2.8725e+00, 2.0016e+00, 6.6109e+00, 2.2014e-01, 2.0070e+01, 3.7864e+00,\n",
       "                      6.3005e-32, 4.3917e+00, 5.6083e+00, 5.9269e-02, 1.0519e+01, 1.0777e+00,\n",
       "                      9.4902e+00, 4.2083e+00, 8.4500e-04, 3.3992e+00, 1.8244e-01, 8.4780e+00,\n",
       "                      6.3717e-03, 1.6412e+01, 3.0136e-01, 3.8194e+00, 5.4095e+00, 1.2527e+01,\n",
       "                      2.3735e+00, 5.9544e+00, 3.1529e-04, 1.1799e+01, 9.3146e+00, 1.7292e+01,\n",
       "                      7.6631e+00, 1.4681e+01, 7.1181e+00, 1.7907e+00, 1.4700e+01, 5.0464e-01,\n",
       "                      6.6291e+00, 5.6052e-45, 1.1566e-02, 8.2039e+00, 3.6604e+00, 7.3399e-08,\n",
       "                      1.1575e+01, 1.1924e+01, 1.3426e+01, 6.5891e-05, 1.2444e+01, 9.3835e-01,\n",
       "                      1.1395e+01, 2.6303e-02, 9.9476e+00, 1.6125e+01, 9.1619e+00, 2.9669e-01,\n",
       "                      3.6965e+00, 4.2634e-31, 1.4316e-04, 2.2650e+00, 1.6570e+01, 4.2288e-02,\n",
       "                      1.2602e-04, 2.6948e+00, 1.5113e+01, 1.7379e+00, 8.2209e+00, 3.5482e+00,\n",
       "                      3.8546e+00, 1.3065e-02, 5.3832e+00, 8.2578e+00, 2.4311e+00, 2.9165e-05,\n",
       "                      1.6332e+01, 5.3222e-01, 1.2481e+01, 9.0217e+00, 1.2600e+01, 1.0933e-04,\n",
       "                      2.6878e+00, 5.0625e+00, 2.6069e+00, 6.1842e+00, 1.6833e+01, 4.2038e+00,\n",
       "                      5.6052e-45, 1.2344e-15, 2.1169e+00, 8.6021e-01, 3.6814e+00, 3.5224e+00,\n",
       "                      1.2887e-30, 5.3769e-01, 4.1917e-02, 3.5554e+00, 2.0164e+01, 5.5988e+00,\n",
       "                      1.4096e+00, 4.9360e+00, 4.6586e-01, 1.0177e+01, 1.9292e-02, 7.0170e+00,\n",
       "                      2.0395e-01, 1.2034e+01, 3.1686e+01, 4.0485e+00, 2.0637e+00, 9.4377e+00,\n",
       "                      7.0068e-06, 4.4849e-01, 1.5771e-05, 7.0569e+00, 1.2667e+01, 1.0285e-03,\n",
       "                      1.4761e+01, 1.0139e+00, 1.3442e+01, 8.5119e+00, 2.3276e+00, 5.3019e-14,\n",
       "                      7.6222e-24, 1.1761e+01, 1.1738e+00, 1.0726e+01, 6.8823e+00, 5.1740e+00,\n",
       "                      1.8972e-02, 9.8891e+00, 2.1739e+01, 7.8830e-31, 5.6052e-45, 2.7209e+00,\n",
       "                      5.4430e-27, 1.0384e-01, 2.5515e+00, 2.0792e+01, 3.7421e-03, 6.0158e+00,\n",
       "                      8.2053e+00, 3.2356e-05, 2.1773e+01, 6.2305e+00, 4.5363e+00, 2.9748e+01,\n",
       "                      5.5000e+00, 2.9206e+01, 2.2456e+01, 2.1966e+01, 1.5763e+01, 7.2850e+00,\n",
       "                      4.5700e+00, 4.2062e-29, 1.6005e+00, 6.0973e+00, 3.9524e-01, 1.2250e-01,\n",
       "                      1.0796e+01, 4.9046e+00, 1.8870e-05, 4.3849e+00, 5.6052e-45, 3.7047e+01,\n",
       "                      9.0373e+00, 6.5948e-25, 5.3023e+00, 3.0337e+00, 3.0219e-02, 9.3206e-01,\n",
       "                      5.3626e+00, 5.6052e-45, 1.2419e+01, 1.9676e-04, 2.2580e+01, 2.6018e-03,\n",
       "                      5.6052e-45, 1.3295e+01, 2.8349e+00, 3.9080e-05, 4.6069e-01, 6.7622e+00,\n",
       "                      1.9349e+01, 3.0503e-01, 1.1224e+00, 4.2494e+00, 1.1331e+01, 1.3020e+00,\n",
       "                      1.3317e+01, 5.6052e-45, 1.7023e-01, 1.7631e+00, 1.0837e+01, 1.6230e+01,\n",
       "                      6.7269e+00, 1.2479e-05, 1.8516e-06, 3.0213e+00, 2.1149e+01, 3.1434e-01,\n",
       "                      3.5126e+01, 4.3334e-01, 1.1498e+01, 7.8290e-01, 2.4232e+01, 4.5011e+00,\n",
       "                      1.2154e+01, 8.9911e+00, 2.4660e+00, 2.0426e+00, 4.1795e+00, 4.2120e-03,\n",
       "                      5.8580e+00, 1.2564e+00, 2.6858e+00, 8.6209e-05, 2.6661e+00, 5.0640e+00,\n",
       "                      2.4875e+01, 9.4219e+00, 5.7363e+00, 1.1962e-01], device='cuda:0')),\n",
       "             ('1.bn.running_var',\n",
       "              tensor([3.1758e-04, 1.8074e+01, 7.8778e-03, 8.5894e-01, 2.3807e+01, 8.4485e-03,\n",
       "                      8.9612e+00, 8.1287e+00, 2.5103e+01, 1.9223e+02, 2.7687e+02, 6.5984e+01,\n",
       "                      6.0647e-05, 7.3713e+01, 2.6547e+01, 4.9834e+01, 7.1123e+01, 5.1010e+01,\n",
       "                      3.0483e+02, 1.1533e+02, 6.1533e+01, 3.9524e+02, 2.7850e+01, 2.0947e+02,\n",
       "                      8.7989e+01, 5.6052e-45, 2.1283e+01, 2.0841e+02, 2.3125e+01, 5.5224e+01,\n",
       "                      2.0371e+02, 1.5342e+02, 2.0618e+02, 6.2158e+01, 6.8364e+01, 4.8395e+01,\n",
       "                      5.1535e+01, 1.1557e+01, 1.0725e+02, 1.0784e+00, 3.5464e+02, 4.0067e+01,\n",
       "                      2.3713e-32, 6.4894e+01, 1.0345e+02, 2.5777e-01, 1.4520e+02, 6.3719e+00,\n",
       "                      1.5420e+02, 6.7572e+01, 1.3285e-03, 2.5983e+01, 7.9516e-01, 1.8371e+02,\n",
       "                      2.8631e-02, 3.0097e+02, 2.0806e+00, 4.5444e+01, 3.7922e+01, 1.2507e+02,\n",
       "                      2.2849e+01, 5.3870e+01, 5.5454e-04, 1.5563e+02, 1.3412e+02, 3.2893e+02,\n",
       "                      9.1368e+01, 1.9157e+02, 1.1854e+02, 2.7301e+01, 1.8200e+02, 3.0527e+00,\n",
       "                      5.8720e+01, 5.6052e-45, 7.5438e-02, 1.0578e+02, 5.3931e+01, 7.1454e-07,\n",
       "                      1.3257e+02, 1.3192e+02, 2.9876e+02, 2.7295e-04, 2.1679e+02, 6.0076e+00,\n",
       "                      8.1037e+01, 1.2370e-01, 1.0612e+02, 3.5902e+02, 1.5575e+02, 2.3122e+00,\n",
       "                      6.4382e+01, 7.0153e-32, 1.7240e-04, 1.6954e+01, 4.4192e+02, 2.5629e-01,\n",
       "                      2.7827e-04, 2.3692e+01, 1.1134e+02, 2.4875e+01, 1.0816e+02, 4.8277e+01,\n",
       "                      4.3216e+01, 3.9179e-02, 7.1523e+01, 6.9559e+01, 2.1281e+01, 1.4468e-05,\n",
       "                      9.3532e+01, 5.4431e+00, 2.7979e+02, 2.2760e+02, 1.1940e+02, 1.7525e-04,\n",
       "                      1.9932e+01, 5.2510e+01, 2.8189e+01, 9.3339e+01, 1.9408e+02, 4.9022e+01,\n",
       "                      5.6052e-45, 5.1910e-16, 2.1566e+01, 6.3577e+00, 4.7538e+01, 2.2216e+01,\n",
       "                      1.3283e-30, 4.0221e+00, 1.4979e-01, 3.2073e+01, 3.5309e+02, 7.4597e+01,\n",
       "                      1.0660e+01, 6.0976e+01, 2.6513e+00, 1.0081e+02, 1.0375e-01, 1.8523e+02,\n",
       "                      1.2901e+00, 1.6685e+02, 1.7995e+02, 3.9399e+01, 2.3368e+01, 7.0794e+01,\n",
       "                      1.3110e-05, 3.0603e+00, 4.3556e-05, 1.0191e+02, 2.6633e+02, 1.2747e-03,\n",
       "                      1.9539e+02, 9.2496e+00, 2.2095e+02, 9.7960e+01, 2.4895e+01, 3.2880e-14,\n",
       "                      1.6565e-23, 1.5437e+02, 8.9392e+00, 1.1158e+02, 1.0461e+02, 2.5888e+01,\n",
       "                      1.1062e-01, 9.7950e+01, 5.3687e+02, 9.8518e-31, 5.6052e-45, 1.9710e+01,\n",
       "                      2.0234e-27, 6.2146e-01, 2.2814e+01, 1.4871e+02, 1.3315e-02, 3.8470e+01,\n",
       "                      8.9287e+01, 1.3103e-05, 2.0633e+02, 7.9883e+01, 4.6109e+01, 4.3428e+02,\n",
       "                      4.2538e+01, 2.8573e+02, 2.0901e+02, 6.1843e+01, 7.3885e+01, 9.3789e+01,\n",
       "                      6.0997e+01, 1.7092e-29, 1.4256e+01, 7.0513e+01, 2.2330e+00, 6.3202e-01,\n",
       "                      1.2592e+02, 5.1294e+01, 1.5641e-05, 6.8649e+01, 5.6052e-45, 2.9363e+02,\n",
       "                      1.4649e+02, 3.9515e-25, 7.4939e+01, 3.3461e+01, 1.1781e-01, 6.9128e+00,\n",
       "                      5.5143e+01, 5.6052e-45, 2.1963e+02, 2.1516e-04, 1.7607e+02, 1.3217e-02,\n",
       "                      5.6052e-45, 1.6485e+02, 4.3346e+01, 5.5091e-05, 2.9131e+00, 7.8885e+01,\n",
       "                      1.4888e+02, 2.7860e+00, 1.0604e+01, 5.1993e+01, 7.8617e+01, 1.3379e+01,\n",
       "                      1.1273e+02, 5.6052e-45, 1.2268e+00, 1.5197e+01, 1.3317e+02, 1.4749e+02,\n",
       "                      9.1350e+01, 6.9348e-06, 3.5788e-06, 4.1637e+01, 1.9025e+02, 2.4250e+00,\n",
       "                      1.6699e+02, 2.7001e+00, 1.7349e+02, 5.6303e+00, 2.1150e+02, 6.5577e+01,\n",
       "                      1.2625e+02, 1.6815e+02, 2.9475e+01, 1.7947e+01, 4.9454e+01, 1.8683e-02,\n",
       "                      6.4038e+01, 1.2232e+01, 2.8014e+01, 4.1662e-04, 4.2395e+01, 3.1477e+01,\n",
       "                      4.3114e+02, 1.0072e+02, 6.6777e+01, 6.3683e-01], device='cuda:0')),\n",
       "             ('1.bn.num_batches_tracked', tensor(6000, device='cuda:0')),\n",
       "             ('2.conv.weight', tensor([[[ 0.4359,  0.2146, -0.0284],\n",
       "                       [ 1.1017,  0.7501,  0.2883],\n",
       "                       [ 0.6941,  0.3055, -0.0597],\n",
       "                       ...,\n",
       "                       [-0.0454,  0.0782,  0.0270],\n",
       "                       [-0.4977, -1.2104, -0.8120],\n",
       "                       [-0.4475, -0.5677, -0.1547]],\n",
       "              \n",
       "                      [[-0.5713, -0.0183, -0.4972],\n",
       "                       [-0.2828, -0.4938, -0.3700],\n",
       "                       [-0.1614, -0.0655,  0.1386],\n",
       "                       ...,\n",
       "                       [-0.3904,  0.0600,  0.2867],\n",
       "                       [ 0.1059, -0.3614, -0.3794],\n",
       "                       [-0.3609, -0.1304, -0.1131]],\n",
       "              \n",
       "                      [[ 0.3061,  0.0757, -0.1533],\n",
       "                       [-0.3938, -0.2053, -0.0565],\n",
       "                       [ 0.4234,  0.3671, -0.0382],\n",
       "                       ...,\n",
       "                       [-0.3353, -0.5070, -0.0191],\n",
       "                       [-0.4884, -0.0672, -0.1056],\n",
       "                       [-0.2066, -0.3937,  0.3926]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.2050, -0.2862, -0.0613],\n",
       "                       [ 0.7003,  0.2751,  0.3007],\n",
       "                       [ 0.5300, -0.3920, -0.2887],\n",
       "                       ...,\n",
       "                       [-0.2541, -0.3732, -0.5563],\n",
       "                       [ 0.5004,  0.4860,  0.1332],\n",
       "                       [ 0.3581,  0.5204,  0.2197]],\n",
       "              \n",
       "                      [[ 0.0415,  0.0182,  0.1335],\n",
       "                       [ 0.5329,  0.4972,  0.7372],\n",
       "                       [-0.5001, -0.2482, -0.4643],\n",
       "                       ...,\n",
       "                       [-0.1873, -0.0454,  0.0749],\n",
       "                       [-0.3866,  0.1786, -0.1750],\n",
       "                       [ 0.2246,  0.2217,  0.0416]],\n",
       "              \n",
       "                      [[-0.1279, -0.4668, -0.4973],\n",
       "                       [ 0.0766,  0.1054,  0.2493],\n",
       "                       [-0.1020, -0.4362,  0.1226],\n",
       "                       ...,\n",
       "                       [-0.1445, -0.0298, -0.2484],\n",
       "                       [-0.1581, -0.5751, -0.7137],\n",
       "                       [ 0.2772,  0.1232, -0.0037]]], device='cuda:0')),\n",
       "             ('2.conv.bias',\n",
       "              tensor([-0.1294, -0.3868,  0.0515,  0.4524,  0.2933, -0.4350, -0.2262,  0.3039,\n",
       "                       0.0404,  0.1268,  0.2851, -0.0074, -0.0930,  0.3920,  0.2039,  0.2784,\n",
       "                       0.2863, -0.0611,  0.3180, -0.1933,  0.0680,  0.4314, -0.5245, -0.7874,\n",
       "                      -0.6641, -0.3413, -0.4630, -0.0031, -0.1580, -0.3676,  0.4644,  0.1054,\n",
       "                      -0.2814,  0.0885,  0.2693, -0.3411, -0.0530,  0.3378, -0.0248, -0.1888,\n",
       "                      -0.8053,  0.2139, -0.6765,  0.0226, -0.0902, -0.1961,  0.3319,  0.2856,\n",
       "                      -0.2947,  0.1212, -0.4915,  0.4463, -0.5659,  0.3969, -0.1593,  0.4218,\n",
       "                       0.0200, -0.4498,  0.1273,  0.4035,  0.3113,  0.1012, -0.3854,  0.3594,\n",
       "                      -0.1379,  0.2256, -0.9783, -0.6328,  0.4300,  0.0030,  0.1542,  0.1756,\n",
       "                      -0.4282, -0.0481, -0.1345, -0.2194, -0.4782, -0.3018, -0.0767,  0.1480,\n",
       "                      -0.2901, -0.5040, -0.5144,  0.3510,  0.0810,  0.4000, -0.6645, -0.3146,\n",
       "                      -0.0852,  0.0782, -0.1076, -0.3242, -0.9832,  0.6424,  0.5395,  0.8531,\n",
       "                       0.2417, -0.2547,  0.0357, -0.8037, -0.0532,  0.6009, -0.2126,  0.4147,\n",
       "                      -0.4049,  0.3297, -0.1643,  0.1721, -0.0688, -0.2446,  0.0324, -0.3001,\n",
       "                       0.4840,  0.0299, -0.6417,  0.0522, -0.3898, -0.0078,  0.0236,  0.1830,\n",
       "                      -0.0539,  0.5797,  0.2915,  0.0473, -0.2146, -0.2315, -0.2449, -0.1368],\n",
       "                     device='cuda:0')),\n",
       "             ('2.bn.weight',\n",
       "              tensor([ 1.0557e+00,  7.6784e-01, -1.3031e-01,  7.9363e-01,  6.8881e-01,\n",
       "                       2.0518e+00,  1.9180e+00,  2.0297e+00,  1.9919e+00, -2.3825e-01,\n",
       "                       1.3692e+00,  6.6884e-01,  4.7439e-01,  1.3016e+00,  1.3000e+00,\n",
       "                       1.3075e+00,  4.3090e-02,  2.2969e+00,  3.2621e-01,  9.4967e-01,\n",
       "                       1.7390e+00, -7.0817e-01,  1.6122e+00,  1.2119e+00,  7.5328e-01,\n",
       "                       1.2381e+00,  3.8411e-01,  1.2488e+00,  3.2929e-01,  9.5522e-01,\n",
       "                       9.2627e-01,  1.2115e+00,  1.4312e+00,  6.1550e-01,  8.9840e-01,\n",
       "                       1.3169e+00,  9.8748e-01,  6.7116e-01,  1.5076e+00,  1.8089e+00,\n",
       "                       1.3379e+00,  1.2967e+00,  7.0268e-01,  1.0609e+00,  1.3799e+00,\n",
       "                       2.3413e-01,  2.0108e+00,  8.3980e-01,  9.2934e-01,  6.3938e-01,\n",
       "                       8.1937e-01,  2.1363e+00,  1.2714e+00,  6.9140e-01,  7.4594e-01,\n",
       "                       1.3763e+00,  4.6215e-01,  1.3650e+00,  2.1002e+00,  1.6500e+00,\n",
       "                      -2.2594e-01,  1.4997e+00,  1.5410e+00,  2.2662e+00,  7.6406e-01,\n",
       "                       9.9784e-01,  1.7253e+00,  1.5247e+00,  1.2108e+00,  8.6926e-01,\n",
       "                       1.3792e+00,  9.5451e-01,  1.3471e+00,  1.8671e+00,  6.1259e-01,\n",
       "                       6.9020e-01,  3.8146e-01,  1.0964e+00,  1.3385e+00,  2.0867e+00,\n",
       "                       1.4667e+00,  8.5362e-01,  3.5865e-01,  3.2168e-01,  7.0635e-01,\n",
       "                      -9.3796e-03,  3.7160e-01,  1.1179e-01,  1.1338e+00,  3.6248e-01,\n",
       "                       3.4037e-01,  1.7883e+00,  1.2754e+00,  1.1015e+00,  7.3559e-01,\n",
       "                       4.5770e-01,  8.5098e-01,  1.4418e+00,  1.1472e+00,  9.1867e-01,\n",
       "                       2.0492e+00,  1.9604e+00,  1.8055e+00,  8.5312e-01,  2.0615e+00,\n",
       "                       1.4651e+00,  8.6076e-01,  5.1080e-01,  8.2631e-01,  2.0659e+00,\n",
       "                       9.6263e-01,  2.2247e+00,  1.3740e+00,  1.3010e+00,  9.6798e-01,\n",
       "                      -3.2378e-04,  8.7247e-01,  1.9237e+00,  2.0857e-01,  8.2280e-01,\n",
       "                       1.1016e+00,  4.1249e-01,  8.3528e-01,  1.2540e+00,  5.6916e-01,\n",
       "                       1.1004e+00,  1.1098e+00,  5.7314e-02], device='cuda:0')),\n",
       "             ('2.bn.bias',\n",
       "              tensor([ 1.3903e-02, -2.9351e-02,  1.7991e-02,  6.4704e-04,  5.7462e-03,\n",
       "                       2.8649e-02, -3.1181e-03, -6.1379e-03,  1.9952e-03,  2.5102e-03,\n",
       "                       5.5976e-03, -4.1172e-03, -1.5755e-03,  2.2776e-02, -3.2995e-03,\n",
       "                      -7.5588e-04,  6.5143e-04, -4.8032e-03, -3.0903e-02, -1.3240e-02,\n",
       "                      -3.2153e-02,  5.1021e-03, -5.5274e-03,  5.3099e-03, -8.6509e-05,\n",
       "                      -8.4438e-06, -4.6401e-03, -5.4635e-03, -7.6510e-03,  5.8818e-03,\n",
       "                       1.6067e-02, -3.5686e-03,  1.3516e-04, -6.5099e-04, -8.1372e-03,\n",
       "                       4.6298e-03, -6.8834e-04,  1.5016e-02, -4.3740e-03,  1.5622e-03,\n",
       "                      -2.2596e-02, -2.1202e-02,  1.2394e-03, -1.6852e-02, -8.8556e-03,\n",
       "                       6.5383e-03, -2.3415e-02, -1.8460e-02, -2.0355e-03, -1.7273e-03,\n",
       "                       5.3908e-03,  1.0754e-02, -1.0528e-03,  3.7591e-03,  2.3902e-02,\n",
       "                       1.2917e-03,  6.5737e-03,  7.3793e-04, -9.0567e-03, -8.8242e-03,\n",
       "                      -3.7208e-03,  1.3192e-02,  6.5928e-03, -1.4549e-02, -7.3012e-03,\n",
       "                       9.1975e-03,  1.3266e-03,  2.5468e-02,  1.0183e-02,  1.3264e-02,\n",
       "                       7.9345e-03, -1.0199e-02,  3.0396e-02,  5.5927e-03,  2.3212e-02,\n",
       "                      -1.5943e-03,  1.8477e-02, -6.0287e-03, -3.8546e-03,  7.7948e-04,\n",
       "                      -9.4789e-05,  1.1516e-04, -7.0154e-04,  8.1971e-03, -2.5441e-02,\n",
       "                      -5.1397e-04,  4.8972e-04, -5.1809e-03,  7.2067e-03, -1.8634e-02,\n",
       "                      -6.1924e-04, -1.0536e-02,  2.1925e-03,  1.3366e-02,  1.4911e-03,\n",
       "                       1.9350e-03, -9.5114e-03,  1.1093e-04, -1.0476e-02, -1.1656e-02,\n",
       "                      -1.6316e-02, -1.1305e-03, -2.0580e-02,  6.1734e-03,  2.9464e-03,\n",
       "                       2.9300e-02, -3.9460e-05, -5.8798e-03,  9.7501e-03, -4.6303e-03,\n",
       "                       8.8836e-03, -6.2540e-02, -1.7120e-02,  1.3610e-02, -5.6134e-03,\n",
       "                      -5.3841e-04, -6.0786e-03, -5.7506e-03,  4.7143e-03, -2.7791e-03,\n",
       "                      -1.2624e-04, -4.7138e-03,  1.1788e-02, -9.4863e-03, -2.0149e-02,\n",
       "                      -2.7012e-02, -6.5881e-03, -3.5183e-04], device='cuda:0')),\n",
       "             ('2.bn.running_mean',\n",
       "              tensor([8.8219e+00, 4.9611e-01, 2.0466e+01, 1.5598e+01, 1.5550e+00, 1.6167e-08,\n",
       "                      1.0810e+00, 1.5174e+01, 1.0922e+01, 1.3836e+01, 2.0738e+01, 8.6755e+00,\n",
       "                      1.2290e-01, 2.2400e+01, 1.7975e+01, 4.0949e+00, 6.4076e-01, 1.2223e+00,\n",
       "                      2.1746e+01, 1.3089e+00, 1.0513e+01, 2.4066e+01, 5.6052e-45, 1.1672e-04,\n",
       "                      5.6052e-45, 5.6052e-45, 1.8888e-03, 1.9955e+01, 1.0906e-01, 5.6052e-45,\n",
       "                      2.0942e+01, 1.4841e+01, 2.7975e-35, 1.3259e-35, 6.3977e+00, 3.1410e-03,\n",
       "                      3.1477e+00, 4.4466e+00, 8.6189e-04, 2.1565e-05, 7.7671e-04, 4.0597e+00,\n",
       "                      8.8308e-12, 6.5025e-01, 1.2020e+00, 1.3667e-03, 1.9092e+00, 1.9984e+01,\n",
       "                      1.9297e+00, 1.2255e+01, 1.0352e-26, 1.4183e+01, 6.9073e-10, 1.0740e+01,\n",
       "                      5.3672e-02, 1.8526e+01, 1.0710e+01, 2.7766e-37, 5.3996e+00, 1.8375e+01,\n",
       "                      1.4122e+01, 1.6099e+01, 1.9866e-01, 2.1886e+01, 4.5889e+00, 1.6892e+01,\n",
       "                      6.9804e-04, 7.0677e-14, 1.6425e+01, 1.8085e-02, 1.2314e+00, 6.3922e-01,\n",
       "                      1.1743e+00, 8.2025e+00, 4.5104e-01, 1.1508e+01, 2.9175e-03, 7.3068e-02,\n",
       "                      4.5716e+00, 1.0603e+01, 5.6052e-45, 5.6052e-45, 2.9798e-04, 1.4405e+01,\n",
       "                      1.6697e+00, 1.2883e+01, 5.6052e-45, 3.9629e+00, 3.1558e-26, 4.2622e+00,\n",
       "                      4.0943e+00, 6.1517e-01, 1.1268e-02, 2.3126e+01, 1.2298e+01, 3.0579e+01,\n",
       "                      1.3880e+01, 6.0103e-09, 4.8339e+00, 2.1496e+00, 4.6763e-01, 1.8900e+01,\n",
       "                      3.0858e+00, 1.7142e+01, 3.0568e-35, 1.6206e+01, 5.6052e-45, 9.3936e+00,\n",
       "                      1.8239e+01, 1.9864e-05, 1.2342e-01, 9.1610e-01, 2.0348e+01, 4.1094e+00,\n",
       "                      8.8328e-28, 4.6936e+00, 3.6071e-04, 3.3226e-03, 5.4671e+00, 1.6643e+01,\n",
       "                      6.6121e+00, 1.6543e+01, 2.4248e+01, 3.6904e+00, 4.5980e-08, 5.8498e-04,\n",
       "                      2.6084e-08, 1.5149e+01], device='cuda:0')),\n",
       "             ('2.bn.running_var',\n",
       "              tensor([1.4562e+02, 1.3940e+01, 1.9373e+02, 1.8905e+02, 2.2210e+01, 2.5206e-07,\n",
       "                      2.3076e+01, 2.0269e+02, 2.2487e+02, 1.2135e+02, 1.8615e+02, 1.3756e+02,\n",
       "                      8.5996e-01, 1.1664e+02, 1.9168e+02, 5.1946e+01, 6.6750e+00, 2.1237e+01,\n",
       "                      6.2255e+01, 1.0438e+01, 1.4696e+02, 2.3629e+02, 5.6052e-45, 1.0134e-03,\n",
       "                      5.6052e-45, 5.6052e-45, 9.3415e-03, 1.3501e+02, 2.2895e+00, 5.6052e-45,\n",
       "                      1.3361e+02, 3.0285e+02, 1.0802e-34, 6.6714e-35, 7.6558e+01, 4.9816e-02,\n",
       "                      5.5994e+01, 4.2381e+01, 4.4266e-03, 1.0024e-04, 3.9015e-03, 5.0140e+01,\n",
       "                      7.1399e-12, 1.1296e+01, 3.4167e+01, 8.5891e-03, 2.9330e+01, 9.2224e+01,\n",
       "                      6.8630e+01, 8.9244e+01, 3.1881e-26, 1.9836e+02, 2.2861e-09, 1.2364e+02,\n",
       "                      3.9068e-01, 1.9816e+02, 1.2538e+02, 7.6691e-37, 8.3200e+01, 3.1528e+02,\n",
       "                      1.5332e+02, 2.6096e+02, 1.5370e+00, 2.3533e+02, 5.8447e+01, 1.8400e+02,\n",
       "                      9.9132e-03, 8.1062e-13, 1.6559e+02, 1.8465e-01, 1.4555e+01, 1.1418e+01,\n",
       "                      1.3988e+01, 1.4855e+02, 6.2370e+00, 1.7211e+02, 2.7997e-02, 1.4975e+00,\n",
       "                      4.3064e+01, 1.2884e+02, 5.6052e-45, 5.6052e-45, 3.5901e-03, 1.6787e+02,\n",
       "                      1.5450e+01, 1.0098e+02, 5.6052e-45, 3.7148e+01, 4.0023e-26, 8.1232e+01,\n",
       "                      5.3813e+01, 9.7819e+00, 1.9481e-01, 1.8421e+02, 1.7781e+02, 2.3242e+02,\n",
       "                      1.4394e+02, 5.4317e-08, 4.8917e+01, 2.5402e+01, 9.6005e+00, 1.6610e+02,\n",
       "                      4.3071e+01, 2.0618e+02, 1.7450e-35, 8.8822e+01, 5.6052e-45, 1.6251e+02,\n",
       "                      1.6484e+02, 1.0500e-04, 1.0580e+00, 1.5282e+01, 1.9387e+02, 6.0368e+01,\n",
       "                      1.3958e-27, 3.2351e+01, 1.4637e-03, 1.4350e-02, 7.5115e+01, 1.1190e+02,\n",
       "                      1.2778e+02, 1.5086e+02, 1.0110e+02, 6.6556e+01, 8.8111e-08, 2.8086e-03,\n",
       "                      4.8187e-08, 1.2561e+02], device='cuda:0')),\n",
       "             ('2.bn.num_batches_tracked', tensor(6000, device='cuda:0'))])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNetwork(\"Wine\",w,lr=1e-2,epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
